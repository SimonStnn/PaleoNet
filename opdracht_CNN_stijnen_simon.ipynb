{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60e89c",
   "metadata": {},
   "source": [
    "AI Deep Learning – Simon Stijnen – May 2025\n",
    "\n",
    "---\n",
    "\n",
    "# Dinosaur Species Classification using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements a CNN model to classify dinosaur species using image data from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93aa31",
   "metadata": {},
   "source": [
    "## 0. Download and Setup Kaggle Dataset\n",
    "\n",
    "In this section, we'll download the dinosaur image dataset from Kaggle. You need to have a Kaggle account and API key to download datasets programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install kaggle if not already installed\n",
    "!pip install -q kaggle kagglehub fastbook\n",
    "\n",
    "# Instructions for downloading kaggle.json credentials (run this once)\n",
    "print(\"To download datasets from Kaggle:\")\n",
    "print(\"1. Go to your Kaggle account settings at https://www.kaggle.com/account\")\n",
    "print(\"2. Click on 'Create New API Token' to download your kaggle.json file\")\n",
    "print(\"3. Place the kaggle.json file in .kaggle/\")\n",
    "print(\"4. Run the cells below to download the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "\n",
    "# Check if kaggle.json exists\n",
    "kaggle_path = os.path.expanduser('.kaggle/kaggle.json')\n",
    "if os.path.exists(kaggle_path):\n",
    "    print(\"Kaggle API credentials found!\")\n",
    "else:\n",
    "    print(\"Kaggle API credentials not found. Please follow the instructions above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the download path for the dataset - using a directory with user permissions\n",
    "DOWNLOAD_PATH = os.path.join(\"data\")\n",
    "\n",
    "# Create download directory if it doesn't exist\n",
    "if not os.path.exists(DOWNLOAD_PATH):\n",
    "    os.makedirs(DOWNLOAD_PATH)\n",
    "\n",
    "import zipfile\n",
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "# Download latest version\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"larserikrisholm/dinosaur-image-dataset-15-species\")\n",
    "    print(\"Dataset downloaded to:\", path)\n",
    "    \n",
    "    # Copy to our data directory instead of moving\n",
    "    zip_dest = os.path.join(DOWNLOAD_PATH, os.path.basename(path))\n",
    "    shutil.copy2(path, zip_dest)\n",
    "    print(f\"Dataset copied to: {zip_dest}\")\n",
    "    \n",
    "    # Extract the dataset\n",
    "    extract_folder = os.path.join(DOWNLOAD_PATH, \"dinosaur-dataset\")\n",
    "    if not os.path.exists(extract_folder):\n",
    "        os.makedirs(extract_folder)\n",
    "        \n",
    "    with zipfile.ZipFile(zip_dest, 'r') as zip_ref:\n",
    "        print(f\"Extracting to {extract_folder}...\")\n",
    "        zip_ref.extractall(extract_folder)\n",
    "    print(\"Dataset extracted successfully!\")\n",
    "    \n",
    "    # Update dataset path\n",
    "    DATASET_PATH = extract_folder\n",
    "    print(f\"Dataset path set to: {DATASET_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading or extracting dataset: {e}\")\n",
    "    print(\"Please download the dataset manually from https://www.kaggle.com/datasets/larserikrisholm/dinosaur-image-dataset-15-species\")\n",
    "    print(\"Extract it to the 'data/dinosaur_dataset' folder.\")\n",
    "    DATASET_PATH = os.path.join(DOWNLOAD_PATH, \"dinosaur_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09452e53",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Dataset\n",
    "\n",
    "In this section, we'll load the Kaggle dataset containing dinosaur images, explore its structure, and visualize some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615bdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Configure plot settings\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845db213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset is available\n",
    "print(DATASET_PATH)\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"Dataset not found. Please download the dataset from Kaggle and extract it to the {DATASET_PATH} folder.\")\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}\")\n",
    "\n",
    "print(f\"Dataset found at {DATASET_PATH}!\")\n",
    "# List the contents of the dataset directory\n",
    "print(\"\\nDataset structure:\")\n",
    "for root, dirs, files in os.walk(DATASET_PATH, topdown=True, onerror=None):\n",
    "    level = root.replace(DATASET_PATH, '').count(os.sep)\n",
    "    indent = ' ' * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    for file in files[:5]:  # Show only first 5 files per directory\n",
    "        print(f\"{indent}    {file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{indent}    ... ({len(files) - 5} more files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f28cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_open_image(fn):\n",
    "    try:\n",
    "        _ = Image.open(fn)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_species(fn: str):\n",
    "    \"\"\"Get the species label from the filename.\"\"\"\n",
    "    # Assuming the filename format is 'species_name_1.jpg', 'species_name_2.jpg', etc.\n",
    "    # Adjust the split logic based on your filename format\n",
    "    # Example: 'species_name_1.jpg' -> 'species_name'\n",
    "    label = fn.split(os.sep)[-1].split(\"_\")[0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per class\n",
    "class_counts = {}\n",
    "\n",
    "for class_name in os.listdir(DATASET_PATH):\n",
    "    class_path = os.path.join(DATASET_PATH, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        class_counts[class_name] = len(os.listdir(class_path))\n",
    "        \n",
    "# Create dataframe and plot distribution\n",
    "class_df = pd.DataFrame({\n",
    "    'Dinosaur Species': list(class_counts.keys()),\n",
    "    'Image Count': list(class_counts.values())\n",
    "})\n",
    "\n",
    "class_df = class_df.sort_values('Image Count', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Dinosaur Species', y='Image Count', data=class_df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of Images per Dinosaur Species')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total number of classes: {len(class_counts)}\")\n",
    "print(f\"Total number of images: {sum(class_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "def display_sample_images(dataset_path, num_classes=5, samples_per_class=4):\n",
    "    \"\"\"\n",
    "    Display sample images from random classes in the dataset\n",
    "    \"\"\"\n",
    "    classes = list(os.listdir(dataset_path))\n",
    "    selected_classes = random.sample(classes, min(num_classes, len(classes)))\n",
    "    \n",
    "    fig, axs = plt.subplots(num_classes, samples_per_class, figsize=(12, 10))\n",
    "    \n",
    "    for i, class_name in enumerate(selected_classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            image_files = os.listdir(class_path)\n",
    "            selected_images = random.sample(image_files, min(samples_per_class, len(image_files)))\n",
    "            \n",
    "            for j, img_file in enumerate(selected_images):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                img = Image.open(img_path)\n",
    "                axs[i, j].imshow(img)\n",
    "                axs[i, j].set_title(f\"{class_name}\")\n",
    "                axs[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display sample images\n",
    "display_sample_images(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f91e97",
   "metadata": {},
   "source": [
    "## 2. Preprocess Dataset\n",
    "\n",
    "In this section, we'll preprocess the images by resizing them to a standard size, normalizing pixel values, and encoding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_files(path, recurse=True) -> list[str]:\n",
    "    \"\"\"Get all image files in the dataset directory.\"\"\"\n",
    "    if recurse:\n",
    "        return [os.path.join(root, file) for root, _, files in os.walk(path) for file in files if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    else:\n",
    "        return [os.path.join(path, file) for file in os.listdir(path) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(\"Getting image files...\")\n",
    "\n",
    "all_images = get_image_files(DATASET_PATH, recurse=True)\n",
    "\n",
    "df = pd.DataFrame({'image': all_images, 'species': [get_species(fn) for fn in all_images]})\n",
    "min_count = df['species'].value_counts().min()\n",
    "\n",
    "balanced_df = df.groupby('species').sample(n=min_count, replace=False, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=balanced_df, x='species', order=balanced_df['species'].value_counts().index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Balanced Number of Images per Dinosaur Species')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Balanced dataset size: {len(balanced_df)}\")\n",
    "\n",
    "def get_balanced_image_files(path):\n",
    "    return balanced_df['image'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb726b8",
   "metadata": {},
   "source": [
    "## 3: Create a datablock, dataloaders and train the Model\n",
    "\n",
    "Set up the DataBlock with data augmentation, create a learner, find the optimal learning rate, and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of images in the balanced dataset\n",
    "balanced_files = get_balanced_image_files(path)\n",
    "print(f\"Number of images in the balanced dataset: {len(balanced_files)}\")\n",
    "\n",
    "# If the above number is greater than zero, proceed to create the DataLoaders\n",
    "if len(balanced_files) > 0:\n",
    "    dls = DataBlock(\n",
    "        blocks=(ImageBlock, CategoryBlock),\n",
    "        get_items=get_balanced_image_files,\n",
    "        splitter=RandomSplitter(valid_pct=0.25, seed=42),\n",
    "        get_y=parent_label,\n",
    "        item_tfms=Resize(460),\n",
    "        batch_tfms=[\n",
    "            *aug_transforms(size=224, min_scale=0.75),\n",
    "            Normalize.from_stats(*imagenet_stats),\n",
    "        ],\n",
    "    ).dataloaders(path, bs=32)\n",
    "\n",
    "    # Create a learner\n",
    "    learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "\n",
    "    # Find learning rate\n",
    "    lr_min, lr_steep = learn.lr_find(suggest_funcs=(valley, steep))\n",
    "\n",
    "    # Train the model\n",
    "    try:\n",
    "        learn.fine_tune(6, base_lr=lr_min)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during fine-tuning: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No images found in the balanced dataset. Please check your dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "learn.export('model/dinosaur_classifier.pkl')\n",
    "print(\"Model saved as 'model/dinosaur_classifier.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8c397",
   "metadata": {},
   "source": [
    "## 5. Model Inference on New Images\n",
    "\n",
    "Let's use our trained model to make predictions on new dinosaur images that weren't part of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "try:\n",
    "    learn_inference = load_learner('model/dinosaur_classifier.pkl')\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Training a new model...\")\n",
    "    learn_inference = learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb02193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to make predictions on new images\n",
    "def predict_image(img_path):\n",
    "    \"\"\"Make prediction on a dinosaur image\"\"\"\n",
    "    img = PILImage.create(img_path)\n",
    "    pred_class, pred_idx, probs = learn_inference.predict(img)\n",
    "    return {\n",
    "        'prediction': pred_class,\n",
    "        'probability': float(probs[pred_idx]),\n",
    "        'all_probabilities': {learn_inference.dls.vocab[i]: float(probs[i]) for i in range(len(probs))}\n",
    "    }\n",
    "\n",
    "# Create a function to display predictions on an image\n",
    "def show_prediction(img_path):\n",
    "    \"\"\"Display an image with its prediction\"\"\"\n",
    "    img = PILImage.create(img_path)\n",
    "    pred_data = predict_image(img_path)\n",
    "    \n",
    "    # Display image with prediction\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Prediction: {pred_data['prediction']}\\nProbability: {pred_data['probability']:.2%}\", fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    # Display prediction probabilities\n",
    "    probs_df = pd.DataFrame({\n",
    "        'Species': list(pred_data['all_probabilities'].keys()),\n",
    "        'Probability': list(pred_data['all_probabilities'].values())\n",
    "    }).sort_values('Probability', ascending=False).head(5)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x='Probability', y='Species', data=probs_df)\n",
    "    plt.title('Top 5 Predictions', fontsize=14)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fe8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our model on a few random test images\n",
    "import random\n",
    "\n",
    "# Get random images from different dinosaur species\n",
    "random_test_images = []\n",
    "classes = list(os.listdir(DATASET_PATH))\n",
    "for class_name in random.sample(classes, min(5, len(classes))):\n",
    "    class_path = os.path.join(DATASET_PATH, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        image_files = os.listdir(class_path)\n",
    "        if image_files:\n",
    "            random_img = random.choice(image_files)\n",
    "            random_test_images.append(os.path.join(class_path, random_img))\n",
    "\n",
    "# Make predictions on random test images\n",
    "for img_path in random_test_images:\n",
    "    show_prediction(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9a9da",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Let's evaluate our trained model on the test dataset to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset using FastAI\n",
    "val_loss, val_metrics = learn.validate()\n",
    "\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Error Rate: {val_metrics}\")\n",
    "print(f\"Validation Accuracy: {1 - val_metrics:.4f} ({(1 - val_metrics) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix with more detail\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(12, 12), dpi=100)\n",
    "\n",
    "# Get detailed classification report\n",
    "from sklearn.metrics import classification_report\n",
    "preds, y = learn.get_preds()\n",
    "pred_class = preds.argmax(dim=1)\n",
    "report = classification_report(y.numpy(), pred_class.numpy(), target_names=learn.dls.vocab)\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffe22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the most confused categories\n",
    "interp.most_confused(min_val=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadf6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe771370",
   "metadata": {},
   "source": [
    "## 7. Model Deployment Considerations\n",
    "\n",
    "In this section, we'll discuss how the model could be deployed for real-world applications and potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model class mapping for deployment\n",
    "import json\n",
    "class_mapping = {i: class_name for i, class_name in enumerate(learn.dls.vocab)}\n",
    "\n",
    "# Save mapping to JSON file for later use in deployment\n",
    "with open('model/dinosaur_class_mapping.json', 'w') as f:\n",
    "    json.dump(class_mapping, f)\n",
    "\n",
    "# Save model architecture details\n",
    "model_architecture = {\n",
    "    \"model_type\": \"ResNet34\",\n",
    "    \"pre_trained\": True,\n",
    "    \"num_classes\": len(class_mapping),\n",
    "    \"image_size\": 224,\n",
    "    \"normalization\": \"ImageNet stats\"\n",
    "}\n",
    "\n",
    "with open('model/dinosaur_model_architecture.json', 'w') as f:\n",
    "    json.dump(model_architecture, f)\n",
    "\n",
    "# Save model performance metrics\n",
    "model_performance = {\n",
    "    \"validation_loss\": float(val_loss),\n",
    "    \"validation_error_rate\": float(val_metrics),\n",
    "    \"validation_accuracy\": float(1 - val_metrics),\n",
    "    \"training_date\": \"May 5, 2025\"\n",
    "}\n",
    "\n",
    "with open('model/dinosaur_model_performance.json', 'w') as f:\n",
    "    json.dump(model_performance, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab5060",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Future Improvements\n",
    "\n",
    "In this notebook, we've built a Convolutional Neural Network model for classifying dinosaur species using images. Here's a summary of what we've achieved:\n",
    "\n",
    "1. **Data Loading and Exploration**: We loaded a dataset of dinosaur images, explored their distribution, and visualized sample images from each class.\n",
    "2. **Data Preprocessing**: We preprocessed the images by resizing them and applying data augmentation to help the model generalize better.\n",
    "3. **Model Training**: We fine-tuned a pre-trained ResNet34 model on our dinosaur dataset, achieving good classification accuracy.\n",
    "4. **Evaluation**: We evaluated the model's performance using validation metrics and visualized the confusion matrix to understand where the model makes mistakes.\n",
    "5. **Inference**: We demonstrated how to use the trained model to make predictions on new dinosaur images.\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "1. **More Data**: Collect more dinosaur images to improve model robustness.\n",
    "2. **Advanced Architectures**: Experiment with more advanced CNN architectures like EfficientNet or Vision Transformers.\n",
    "3. **Ensemble Methods**: Combine predictions from multiple models for better accuracy.\n",
    "4. **Extended Augmentation**: Implement more aggressive data augmentation to handle varied image conditions.\n",
    "5. **Transfer Learning**: Explore different pre-trained models and fine-tuning strategies.\n",
    "\n",
    "### Applications:\n",
    "\n",
    "This model could be integrated into educational applications, museum exhibits, or paleontology research tools to help identify dinosaur species from images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
