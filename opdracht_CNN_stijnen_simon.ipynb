{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60e89c",
   "metadata": {},
   "source": [
    "AI Deep Learning – Simon Stijnen – May 2025\n",
    "\n",
    "---\n",
    "\n",
    "# Dinosaur Species Classification using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements a CNN model to classify dinosaur species using image data from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe02ed7",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "In this project, we aim to build a deep learning model capable of distinguishing between 15 different dinosaur species using the [Dinosaur Image Dataset from Kaggle](https://www.kaggle.com/datasets/larserikrisholm/dinosaur-image-dataset-15-species).\n",
    "\n",
    "The main objectives include:\n",
    "\n",
    "1. Splitting the dataset into appropriate training, validation, and test sets\n",
    "2. Selecting an appropriate CNN architecture\n",
    "3. Tuning hyperparameters for optimal performance\n",
    "4. Preventing overfitting with proper regularization techniques\n",
    "5. Using Keras' Functional API to build the model\n",
    "6. Evaluating the model with accuracy metrics and confusion matrices\n",
    "7. Achieving an accuracy greater than 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620356f4",
   "metadata": {},
   "source": [
    "Importing tensorflow and other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0e3f6",
   "metadata": {},
   "source": [
    "Define the path to the dataset and set the image size and batch size for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this path to your extracted dataset location\n",
    "base_dir = 'data/dinosaur_dataset'  # e.g., '/content/drive/MyDrive/dino_data'\n",
    "img_height, img_width = 196, 196\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c050cea",
   "metadata": {},
   "source": [
    "Define the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f666f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.3,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    subset='training',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    subset='validation',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(img_height, img_width, 3))\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "outputs = layers.Dense(15, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0651ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=15,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.reset()\n",
    "preds = model.predict(val_data)\n",
    "predicted_classes = np.argmax(preds, axis=1)\n",
    "true_classes = val_data.classes\n",
    "class_labels = list(val_data.class_indices.keys())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(true_classes, predicted_classes, target_names=class_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
