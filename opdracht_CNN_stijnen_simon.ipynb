{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60e89c",
   "metadata": {},
   "source": [
    "AI Deep Learning – Simon Stijnen – May 2025\n",
    "\n",
    "---\n",
    "\n",
    "# Dinosaur Species Classification using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements a CNN model to classify dinosaur species using image data from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe02ed7",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "In this project, we aim to build a deep learning model capable of distinguishing between 15 different dinosaur species using the [Dinosaur Image Dataset from Kaggle](https://www.kaggle.com/datasets/larserikrisholm/dinosaur-image-dataset-15-species).\n",
    "\n",
    "The main objectives include:\n",
    "\n",
    "1. Splitting the dataset into appropriate training, validation, and test sets\n",
    "2. Selecting an appropriate CNN architecture\n",
    "3. Tuning hyperparameters for optimal performance\n",
    "4. Preventing overfitting with proper regularization techniques\n",
    "5. Using Keras' Functional API to build the model\n",
    "6. Evaluating the model with accuracy metrics and confusion matrices\n",
    "7. Achieving an accuracy greater than 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6dee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print python version\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457cd0e8",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Splitting\n",
    "\n",
    "We'll use the `split-folders` library to properly split our dataset into training, validation, and test sets with a 70%-15%-15% ratio. This ensures we have proper separation for model evaluation and prevents data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb63c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow scikit-learn matplotlib seaborn Pillow\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a79dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kagglehub\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"larserikrisholm/dinosaur-image-dataset-15-species\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cdaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q split-folders\n",
    "\n",
    "import splitfolders\n",
    "\n",
    "# Dataset path\n",
    "input_folder = os.path.join(path, \"dinosaur_dataset\")  # Path to the dataset\n",
    "\n",
    "# Create absolute path for output directory\n",
    "output_split_dir = os.path.abspath(os.path.join(\"data\", \"dinosaur_dataset_split\"))\n",
    "os.makedirs(output_split_dir, exist_ok=True)\n",
    "\n",
    "print(\"output_split_dir:\", output_split_dir)\n",
    "\n",
    "# Split dataset in train (70%), val (15%), test (15%)\n",
    "splitfolders.ratio(\n",
    "    input_folder,\n",
    "    output=output_split_dir,\n",
    "    seed=42,\n",
    "    ratio=(0.7, 0.15, 0.15),\n",
    "    group_prefix=None,\n",
    "    move=False,\n",
    ")\n",
    "\n",
    "print(\"Dataset successfully split into train, validation, and test sets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0e3f6",
   "metadata": {},
   "source": [
    "## Data Configuration\n",
    "\n",
    "Defining the path to the dataset and setting the image size and batch size for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and model parameters\n",
    "img_height, img_width = 296, 296\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c050cea",
   "metadata": {},
   "source": [
    "## Data Loading and Augmentation\n",
    "\n",
    "In this section, we define the training, validation, and test datasets with appropriate data augmentation techniques for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f666f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=applications.efficientnet.preprocess_input,\n",
    "    brightness_range=[0.8, 1.2],  # Add brightness augmentation\n",
    "    shear_range=0.1  # Add shear augmentation\n",
    ")\n",
    "\n",
    "# Only preprocessing for validation and test data (no augmentation)\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "# Load data from the split directories\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    f'{output_split_dir}/train',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_data = val_test_datagen.flow_from_directory(\n",
    "    f'{output_split_dir}/val',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_data = val_test_datagen.flow_from_directory(\n",
    "    f'{output_split_dir}/test',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # Don't shuffle for consistent evaluation\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "print(f\"Number of training samples: {train_data.samples}\")\n",
    "print(f\"Number of validation samples: {val_data.samples}\")\n",
    "print(f\"Number of test samples: {test_data.samples}\")\n",
    "print(f\"Number of classes: {len(train_data.class_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05951643",
   "metadata": {},
   "source": [
    "## Transfer Learning with EfficientNetB0\n",
    "\n",
    "Instead of building a CNN from scratch, we'll use transfer learning with EfficientNetB0 as our base model. This model has been pre-trained on ImageNet and offers better accuracy with relatively low computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model from EfficientNetB0 - a more powerful CNN architecture with good efficiency\n",
    "base_model = applications.EfficientNetB0(\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False,  # Exclude the classification layer\n",
    "    weights='imagenet'  # Use pre-trained weights from ImageNet\n",
    ")\n",
    "\n",
    "# Freeze the base model to prevent its weights from being updated during initial training\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create our model by adding custom layers on top of the base model\n",
    "inputs = Input(shape=(img_height, img_width, 3))\n",
    "x = base_model(inputs, training=False)  # Pass the inputs through the base model\n",
    "x = layers.GlobalAveragePooling2D()(x)  # Global average pooling reduces params & prevents overfitting\n",
    "\n",
    "# Add more capacity to the top layers\n",
    "x = layers.Dense(512, activation='relu')(x)  # Larger dense layer (512 instead of 256)\n",
    "x = layers.BatchNormalization()(x)  # Add batch normalization for better training stability\n",
    "x = layers.Dropout(0.4)(x)  # Slightly lower dropout rate \n",
    "\n",
    "# Add a second dense layer for more representation power\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "outputs = layers.Dense(15, activation='softmax')(x)  # 15 classes output layer\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model with an appropriate learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),  # Start with a higher learning rate\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0651ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add callbacks for better training\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model/best_model_checkpoint.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# First phase: train just the top layers with the base model frozen\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=20,\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9b824",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "After initial training with the base model frozen, we can unfreeze some of the deeper layers of the base model and train them along with our custom top layers. This allows the model to fine-tune the pre-trained features to our specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# EfficientNetB0 has more layers, so we'll gradually unfreeze\n",
    "# First, unfreeze only the last 10% of the base model\n",
    "total_layers = len(base_model.layers)\n",
    "last_10_percent = int(total_layers * 0.1)\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except the last 10%\n",
    "for layer in base_model.layers[:-last_10_percent]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Use a lower learning rate\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print which layers are trainable\n",
    "for layer in model.layers:\n",
    "    print(f\"{layer.name}: {layer.trainable}\")\n",
    "\n",
    "print(f\"Total layers in base model: {total_layers}\")\n",
    "print(f\"Unfreezing last {last_10_percent} layers for fine-tuning\")\n",
    "\n",
    "# Print counts of trainable and non-trainable parameters\n",
    "trainable_count = np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "non_trainable_count = np.sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_count:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n",
    "print(f\"Total parameters: {trainable_count + non_trainable_count:,}\")\n",
    "\n",
    "# Display some trainable layers from the base model\n",
    "print(\"\\nLast 5 layers of base model and their trainable status:\")\n",
    "for layer in base_model.layers[-5:]:\n",
    "    print(f\"  {layer.name}: {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebac264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to address class imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Get class distribution\n",
    "class_counts = np.bincount(train_data.classes)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_data.classes),\n",
    "    y=train_data.classes\n",
    ")\n",
    "\n",
    "# Convert to dictionary format required by Keras\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "print(\"Class weights to handle imbalance:\")\n",
    "for class_name, class_idx in train_data.class_indices.items():\n",
    "    print(f\"  {class_name}: {class_weights[class_idx]:.2f}\")\n",
    "\n",
    "# Second phase: fine-tuning with unfrozen layers\n",
    "fine_tune_history = model.fit(\n",
    "    train_data,\n",
    "    epochs=15,  # Increase epochs with early stopping\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    class_weight=class_weights  # Apply class weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8eda9",
   "metadata": {},
   "source": [
    "## Visualizing Training History\n",
    "\n",
    "Let's plot the training and validation accuracy/loss to see how our model performed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories from both training phases\n",
    "acc = history.history['accuracy'] + fine_tune_history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy'] + fine_tune_history.history['val_accuracy']\n",
    "loss = history.history['loss'] + fine_tune_history.history['loss']\n",
    "val_loss = history.history['val_loss'] + fine_tune_history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([9, 9], [0, 1], 'r--', label='Start Fine Tuning')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.plot([9, 9], [0, 1], 'r--', label='Start Fine Tuning')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187195b",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Set\n",
    "\n",
    "Now we'll evaluate our model on the completely separate test set to get a true measure of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "test_data.reset()  # Reset before predictions\n",
    "y_pred_probs = model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Extract the true labels\n",
    "# Since the test generator doesn't shuffle, labels align with class indices\n",
    "y_true = test_data.classes\n",
    "class_labels = list(test_data.class_indices.keys())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot raw counts confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap='Blues')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix (Counts)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, target_names=class_labels, digits=3)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Calculate overall metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Macro Precision: {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Macro Recall: {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Macro F1-Score: {f1_score(y_true, y_pred, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b0f7a",
   "metadata": {},
   "source": [
    "## Sample Predictions\n",
    "\n",
    "Let's visualize some sample predictions to see how our model performs on individual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_and_display_image(img_path, model, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_batch = np.expand_dims(img_array, axis=0)\n",
    "    preprocessed_img = applications.efficientnet.preprocess_input(img_batch)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(preprocessed_img)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class]\n",
    "    \n",
    "    # Display image and prediction\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Predicted: {class_labels[predicted_class]}\\nConfidence: {confidence:.2f}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show top 3 predictions\n",
    "    top_3_idx = np.argsort(predictions[0])[-3:][::-1]\n",
    "    top_3_classes = [class_labels[i] for i in top_3_idx]\n",
    "    top_3_confidences = [predictions[0][i] for i in top_3_idx]\n",
    "    \n",
    "    for cls, conf in zip(top_3_classes, top_3_confidences):\n",
    "        print(f\"{cls}: {conf:.4f}\")\n",
    "\n",
    "# Get a list of dinosaur classes\n",
    "dino_classes = list(train_data.class_indices.keys())\n",
    "\n",
    "# Define the test directory path\n",
    "test_dir = os.path.join(output_split_dir, 'test')\n",
    "\n",
    "# Sample a few images from different classes for prediction\n",
    "for dino_class in random.sample(dino_classes, 3):\n",
    "    class_dir = os.path.join(test_dir, dino_class)\n",
    "    image_files = os.listdir(class_dir)\n",
    "    sample_image = os.path.join(class_dir, random.choice(image_files))\n",
    "    print(f\"\\nSample from class: {dino_class}\")\n",
    "    predict_and_display_image(sample_image, model, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fd7b0",
   "metadata": {},
   "source": [
    "## Analyze Misclassifications\n",
    "\n",
    "Let's examine some of the misclassified images to understand what might be confusing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77440ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths and true labels from the test directory\n",
    "test_image_paths = []\n",
    "test_labels = []\n",
    "\n",
    "for class_idx, class_name in enumerate(test_data.class_indices):\n",
    "    class_dir = os.path.join(test_dir, class_name)\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            test_image_paths.append(img_path)\n",
    "            test_labels.append(class_idx)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_image_paths = np.array(test_image_paths)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Find misclassified indices\n",
    "misclassified_indices = np.where(y_pred != y_true)[0]\n",
    "print(f\"Total misclassified images: {len(misclassified_indices)} out of {len(y_true)} ({len(misclassified_indices)/len(y_true)*100:.2f}%)\")\n",
    "\n",
    "# Display random misclassified images\n",
    "n_display = min(6, len(misclassified_indices))\n",
    "sample_indices = np.random.choice(misclassified_indices, n_display, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get the file path for this test image\n",
    "    img_idx = idx % len(test_image_paths)  # Handle case where idx is out of range\n",
    "    img_path = test_image_paths[img_idx]\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_batch = np.expand_dims(img_array, axis=0)\n",
    "    preprocessed_img = applications.efficientnet.preprocess_input(img_batch)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(preprocessed_img)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class]\n",
    "    \n",
    "    # Get true class\n",
    "    true_class = y_true[idx]\n",
    "    \n",
    "    # Get top 3 predictions for this image\n",
    "    top_3_idx = np.argsort(predictions[0])[-3:][::-1]\n",
    "    top_3_classes = [class_labels[i] for i in top_3_idx]\n",
    "    top_3_confidences = [predictions[0][i] for i in top_3_idx]\n",
    "    top_3_text = '\\n'.join([f\"{cls}: {conf:.2f}\" for cls, conf in zip(top_3_classes, top_3_confidences)])\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"True: {class_labels[true_class]}\\nPred: {class_labels[predicted_class]}\\nConf: {confidence:.2f}\")\n",
    "    plt.xlabel(top_3_text)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze which classes are most frequently confused\n",
    "confusion_pairs = [(class_labels[y_true[i]], class_labels[y_pred[i]]) for i in misclassified_indices]\n",
    "pairs_count = {}\n",
    "for true_class, pred_class in confusion_pairs:\n",
    "    if (true_class, pred_class) in pairs_count:\n",
    "        pairs_count[(true_class, pred_class)] += 1\n",
    "    else:\n",
    "        pairs_count[(true_class, pred_class)] = 1\n",
    "\n",
    "# Get the top 5 most confused pairs\n",
    "top_confused_pairs = sorted(pairs_count.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"\\nTop confused class pairs (true → predicted):\")\n",
    "for (true_class, pred_class), count in top_confused_pairs:\n",
    "    print(f\"  {true_class} → {pred_class}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00818204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture and weights\n",
    "model.save('model/dinosaur_classifier_transfer_learning.keras')\n",
    "\n",
    "# Save the class labels mapping\n",
    "import json\n",
    "with open('model/dinosaur_class_mapping.json', 'w') as f:\n",
    "    json.dump(test_data.class_indices, f)\n",
    "\n",
    "# Save test performance metrics\n",
    "test_metrics = {\n",
    "    'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "    'precision': float(precision_score(y_true, y_pred, average='macro')),\n",
    "    'recall': float(recall_score(y_true, y_pred, average='macro')),\n",
    "    'f1_score': float(f1_score(y_true, y_pred, average='macro')),\n",
    "    'classes': test_data.class_indices\n",
    "}\n",
    "\n",
    "with open('model/dinosaur_model_performance.json', 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "print(\"Model, class mapping, and performance metrics saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
