{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60e89c",
   "metadata": {},
   "source": [
    "# Dinosaur Species Classification using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements a CNN model to classify dinosaur species using image data from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93aa31",
   "metadata": {},
   "source": [
    "## 0. Download and Setup Kaggle Dataset\n",
    "\n",
    "In this section, we'll download the dinosaur image dataset from Kaggle. You need to have a Kaggle account and API key to download datasets programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install kaggle if not already installed\n",
    "!pip install -q kaggle kagglehub\n",
    "\n",
    "# Instructions for downloading kaggle.json credentials (run this once)\n",
    "print(\"To download datasets from Kaggle:\")\n",
    "print(\"1. Go to your Kaggle account settings at https://www.kaggle.com/account\")\n",
    "print(\"2. Click on 'Create New API Token' to download your kaggle.json file\")\n",
    "print(\"3. Place the kaggle.json file in .kaggle/\")\n",
    "print(\"4. Run the cells below to download the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if kaggle.json exists\n",
    "kaggle_path = os.path.expanduser('.kaggle/kaggle.json')\n",
    "if os.path.exists(kaggle_path):\n",
    "    print(\"Kaggle API credentials found!\")\n",
    "else:\n",
    "    print(\"Kaggle API credentials not found. Please follow the instructions above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the download path for the dataset - using a directory with user permissions\n",
    "DOWNLOAD_PATH = os.path.join(\"data\")\n",
    "\n",
    "# Create download directory if it doesn't exist\n",
    "if not os.path.exists(DOWNLOAD_PATH):\n",
    "    os.makedirs(DOWNLOAD_PATH)\n",
    "\n",
    "import zipfile\n",
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "# Download latest version\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"larserikrisholm/dinosaur-image-dataset-15-species\")\n",
    "    print(\"Dataset downloaded to:\", path)\n",
    "    \n",
    "    # Copy to our data directory instead of moving\n",
    "    zip_dest = os.path.join(DOWNLOAD_PATH, os.path.basename(path))\n",
    "    shutil.copy2(path, zip_dest)\n",
    "    print(f\"Dataset copied to: {zip_dest}\")\n",
    "    \n",
    "    # Extract the dataset\n",
    "    extract_folder = os.path.join(DOWNLOAD_PATH, \"dinosaur-dataset\")\n",
    "    if not os.path.exists(extract_folder):\n",
    "        os.makedirs(extract_folder)\n",
    "        \n",
    "    with zipfile.ZipFile(zip_dest, 'r') as zip_ref:\n",
    "        print(f\"Extracting to {extract_folder}...\")\n",
    "        zip_ref.extractall(extract_folder)\n",
    "    print(\"Dataset extracted successfully!\")\n",
    "    \n",
    "    # Update dataset path\n",
    "    DATASET_PATH = extract_folder\n",
    "    print(f\"Dataset path set to: {DATASET_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading or extracting dataset: {e}\")\n",
    "    print(\"Please download the dataset manually from https://www.kaggle.com/datasets/larserikrisholm/dinosaur-image-dataset-15-species\")\n",
    "    print(\"Extract it to the 'data/dinosaur_dataset' folder.\")\n",
    "    DATASET_PATH = os.path.join(DOWNLOAD_PATH, \"dinosaur_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09452e53",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Dataset\n",
    "\n",
    "In this section, we'll load the Kaggle dataset containing dinosaur images, explore its structure, and visualize some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615bdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# For deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configure plot settings\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845db213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset is available\n",
    "print(DATASET_PATH)\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"Dataset not found. Please download the dataset from Kaggle and extract it to the data/dinosaur-dataset folder.\")\n",
    "else:\n",
    "    print(f\"Dataset found at {DATASET_PATH}!\")\n",
    "    # List the contents of the dataset directory\n",
    "    print(\"\\nDataset structure:\")\n",
    "    for root, dirs, files in os.walk(DATASET_PATH, topdown=True, onerror=None):\n",
    "        level = root.replace(DATASET_PATH, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        for file in files[:5]:  # Show only first 5 files per directory\n",
    "            print(f\"{indent}    {file}\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"{indent}    ... ({len(files) - 5} more files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per class\n",
    "class_counts = {}\n",
    "\n",
    "for class_name in os.listdir(DATASET_PATH):\n",
    "    class_path = os.path.join(DATASET_PATH, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        class_counts[class_name] = len(os.listdir(class_path))\n",
    "\n",
    "# Create dataframe and plot distribution\n",
    "class_df = pd.DataFrame({\n",
    "    'Dinosaur Species': list(class_counts.keys()),\n",
    "    'Image Count': list(class_counts.values())\n",
    "})\n",
    "\n",
    "class_df = class_df.sort_values('Image Count', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Dinosaur Species', y='Image Count', data=class_df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of Images per Dinosaur Species')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total number of classes: {len(class_counts)}\")\n",
    "print(f\"Total number of images: {sum(class_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "def display_sample_images(dataset_path, num_classes=5, samples_per_class=4):\n",
    "    \"\"\"\n",
    "    Display sample images from random classes in the dataset\n",
    "    \"\"\"\n",
    "    classes = list(os.listdir(dataset_path))\n",
    "    selected_classes = random.sample(classes, min(num_classes, len(classes)))\n",
    "    \n",
    "    fig, axs = plt.subplots(num_classes, samples_per_class, figsize=(12, 10))\n",
    "    \n",
    "    for i, class_name in enumerate(selected_classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            image_files = os.listdir(class_path)\n",
    "            selected_images = random.sample(image_files, min(samples_per_class, len(image_files)))\n",
    "            \n",
    "            for j, img_file in enumerate(selected_images):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                img = Image.open(img_path)\n",
    "                axs[i, j].imshow(img)\n",
    "                axs[i, j].set_title(f\"{class_name}\")\n",
    "                axs[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display sample images\n",
    "display_sample_images(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f91e97",
   "metadata": {},
   "source": [
    "## 2. Preprocess Dataset\n",
    "\n",
    "In this section, we'll preprocess the images by resizing them to a standard size, normalizing pixel values, and encoding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image dimensions and batch size\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create image data generators for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.3  # 30% of data will be used for validation and testing\n",
    ")\n",
    "\n",
    "# Use the same preprocessing but no augmentation for validation and test data\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb726b8",
   "metadata": {},
   "source": [
    "## 3. Split Dataset into Training, Validation, and Test Sets\n",
    "\n",
    "We'll split our dataset into training (70%), validation (15%), and test sets (15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (subset='training' takes the first 70% of the data)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data (subset='validation' takes the remaining 30% of the data)\n",
    "temp_val_generator = val_test_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get class indices for label mapping\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = list(class_indices.keys())\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Collect all validation data\n",
    "val_images, val_labels = next(temp_val_generator)\n",
    "\n",
    "# Split validation data into validation and test sets (50/50 split)\n",
    "val_test_split = len(val_images) // 2\n",
    "val_idx = np.random.choice(len(val_images), val_test_split, replace=False)\n",
    "test_idx = np.setdiff1d(np.arange(len(val_images)), val_idx)\n",
    "\n",
    "# Create validation and test datasets\n",
    "validation_images = val_images[val_idx]\n",
    "validation_labels = val_labels[val_idx]\n",
    "test_images = val_images[test_idx]\n",
    "test_labels = val_labels[test_idx]\n",
    "\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {len(validation_images)}\")\n",
    "print(f\"Test samples: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754abb2",
   "metadata": {},
   "source": [
    "## 4. Define CNN Model Using Keras Functional API\n",
    "\n",
    "We'll create a CNN model using the Keras Functional API, leveraging transfer learning with a pre-trained MobileNetV2 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using Keras Functional API with transfer learning\n",
    "def build_model(num_classes):\n",
    "    # Use MobileNetV2 as base model with pre-trained weights\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Create the input layer\n",
    "    inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    # Pass the input through the base model\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Add custom classification layers\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer with softmax activation for multi-class classification\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create the full model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Get number of classes from the generator\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Build the model\n",
    "model, base_model = build_model(num_classes)\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc9b884",
   "metadata": {},
   "source": [
    "## 5. Compile and Train the Model\n",
    "\n",
    "Now we'll compile the model with appropriate loss function and optimizer, and train it while monitoring validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Set up callbacks for training\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_dinosaur_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f291bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=(validation_images, validation_labels),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37fba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model by unfreezing some layers of the base model\n",
    "def fine_tune_model():\n",
    "    # Unfreeze the top layers of the base model\n",
    "    for layer in base_model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Recompile the model with a lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the model again for fine-tuning\n",
    "    fine_tune_history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=(validation_images, validation_labels),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return fine_tune_history\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = fine_tune_model()\n",
    "\n",
    "# Plot fine-tuning history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fine_tune_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(fine_tune_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Fine-tuning Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fine_tune_history.history['loss'], label='Training Loss')\n",
    "plt.plot(fine_tune_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Fine-tuning Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c716ea",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Let's evaluate our trained model on the test dataset to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_dinosaur_model.h5')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = best_model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(test_images)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "# Get the unique classes in y_true_classes\n",
    "unique_classes = np.unique(y_true_classes)\n",
    "\n",
    "# Generate the classification report with the correct labels\n",
    "class_report = classification_report(\n",
    "    y_true_classes, \n",
    "    y_pred_classes, \n",
    "    target_names=[class_names[i] for i in unique_classes],\n",
    "    labels=unique_classes,\n",
    "    digits=4\n",
    ")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f084c",
   "metadata": {},
   "source": [
    "## 7. Generate Confusion Matrix\n",
    "\n",
    "We'll visualize a confusion matrix to see which dinosaur species are frequently misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89067ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    conf_matrix, \n",
    "    annot=True, \n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some misclassifications\n",
    "def plot_misclassifications(x_test, y_true, y_pred, class_names, num_examples=5):\n",
    "    # Find misclassified examples\n",
    "    misclassified = np.where(y_true != y_pred)[0]\n",
    "    \n",
    "    if len(misclassified) == 0:\n",
    "        print(\"No misclassifications found!\")\n",
    "        return\n",
    "    \n",
    "    # Select a random subset\n",
    "    examples = np.random.choice(misclassified, min(num_examples, len(misclassified)), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i, idx in enumerate(examples):\n",
    "        plt.subplot(1, num_examples, i+1)\n",
    "        plt.imshow(x_test[idx])\n",
    "        plt.title(f\"True: {class_names[y_true[idx]]}\\nPred: {class_names[y_pred[idx]]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot some misclassifications\n",
    "plot_misclassifications(\n",
    "    test_images, \n",
    "    y_true_classes, \n",
    "    y_pred_classes, \n",
    "    class_names, \n",
    "    num_examples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe41c77",
   "metadata": {},
   "source": [
    "## 8. Save Results and Model\n",
    "\n",
    "Finally, we'll save our model and results for future use and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MAP = \"model\"\n",
    "PATH_ARCHITECTURE = os.path.join(PATH_MAP, 'dinosaur_model_architecture.json')\n",
    "PATH_MAPPING      = os.path.join(PATH_MAP, 'dinosaur_class_mapping.json')\n",
    "PATH_PERFORMANCE  = os.path.join(PATH_MAP, 'dinosaur_model_performance.json')\n",
    "\n",
    "if not os.path.exists(PATH_MAP):\n",
    "    os.makedirs(PATH_MAP)\n",
    "\n",
    "# Save the model architecture as JSON\n",
    "model_json = model.to_json()\n",
    "with open(PATH_ARCHITECTURE, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save class mapping\n",
    "class_mapping = {v: k for k, v in class_indices.items()}\n",
    "import json\n",
    "with open(PATH_MAPPING, 'w') as f:\n",
    "    json.dump(class_mapping, f)\n",
    "\n",
    "# Save model performance metrics\n",
    "performance_metrics = {\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_loss': float(test_loss)\n",
    "}\n",
    "with open(PATH_PERFORMANCE, 'w') as f:\n",
    "    json.dump(performance_metrics, f)\n",
    "\n",
    "print(\"Model and results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to make predictions on new images\n",
    "def predict_dinosaur(image_path, model, class_mapping):\n",
    "    \"\"\"\n",
    "    Predict dinosaur species from an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model: Trained model\n",
    "        class_mapping: Dictionary mapping class indices to class names\n",
    "    \n",
    "    Returns:\n",
    "        Predicted class and confidence\n",
    "    \"\"\"\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, 0) / 255.0  # Normalize\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    # Get predicted class\n",
    "    predicted_class_idx = np.argmax(prediction[0])\n",
    "    confidence = float(prediction[0][predicted_class_idx])\n",
    "    predicted_class = class_mapping[predicted_class_idx]\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example usage (uncomment to test)\n",
    "# test_image_path = \"path/to/test/image.jpg\"\n",
    "# predicted_species, confidence = predict_dinosaur(test_image_path, best_model, class_mapping)\n",
    "# print(f\"Predicted dinosaur species: {predicted_species}\")\n",
    "# print(f\"Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160eba4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. Loaded and explored a dataset of dinosaur species images\n",
    "2. Preprocessed the images and split them into training, validation, and test sets\n",
    "3. Built a CNN model using the Keras Functional API with transfer learning\n",
    "4. Trained the model and fine-tuned it for better performance\n",
    "5. Evaluated the model and visualized results using confusion matrices\n",
    "6. Saved the model and results for future use\n",
    "\n",
    "The model can be deployed to classify new dinosaur images or further improved with more data or architectural changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
